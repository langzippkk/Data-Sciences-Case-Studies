{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tokenization\n",
    "2. Stop Word Removal\n",
    "3. Negation Handling\n",
    "4. Stemming/Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2592</td>\n",
       "      <td>0</td>\n",
       "      <td>Un-bleeping-believable! Meg Ryan doesn't even ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18359</td>\n",
       "      <td>1</td>\n",
       "      <td>This is a extremely well-made film. The acting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1040</td>\n",
       "      <td>0</td>\n",
       "      <td>Every once in a long while a movie will come a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17262</td>\n",
       "      <td>1</td>\n",
       "      <td>Name just says it all. I watched this movie wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9908</td>\n",
       "      <td>0</td>\n",
       "      <td>This movie succeeds at being one of the most u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  labels                                               text\n",
       "0   2592       0  Un-bleeping-believable! Meg Ryan doesn't even ...\n",
       "1  18359       1  This is a extremely well-made film. The acting...\n",
       "2   1040       0  Every once in a long while a movie will come a...\n",
       "3  17262       1  Name just says it all. I watched this movie wi...\n",
       "4   9908       0  This movie succeeds at being one of the most u..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('..\\\\0.data\\\\raw\\\\imdb_raw_data.csv')\n",
    "print(data.shape)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        0\n",
       "labels    0\n",
       "text      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Any null values?\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Un-bleeping-believable!',\n",
       "  \"Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick.\",\n",
       "  'Hard to believe she was the producer on this dog.',\n",
       "  'Plus Kevin Kline: what kind of suicide trip has his career been on?',\n",
       "  'Whoosh...',\n",
       "  'Banzai!!!',\n",
       "  'Finally this was directed by the guy who did Big Chill?',\n",
       "  'Must be a replay of Jonestown - hollywood style.',\n",
       "  'Wooofff!']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get list of 'list of sentences for each review'.\n",
    "sentences = []\n",
    "for t in data.text:\n",
    "    sentences.append(sent_tokenize(t))\n",
    "sentences[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Un-bleeping-believable']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get list of words\n",
    "wordList = []\n",
    "for textList in sentences:\n",
    "\n",
    "    for line in textList:\n",
    "        tokens = word_tokenize(line)\n",
    "        for word in tokens:\n",
    "            wordList.append(word)\n",
    "\n",
    "wordList[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Case Lowering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['un-bleeping-believable', '!', 'meg', 'ryan', 'does', \"n't\", 'even', 'look', 'her', 'usual']\n"
     ]
    }
   ],
   "source": [
    "lower_tokens = [token.lower() for token in wordList]\n",
    "print(lower_tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "stopList = stopwords.words('english')\n",
    "print(stopList[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7057855\n",
      "4274975\n"
     ]
    }
   ],
   "source": [
    "print(len(wordList))\n",
    "wordList2 = [token for token in lower_tokens if token not in stopList]\n",
    "print(len(wordList2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Negation Handling - Not required now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['un-bleeping-believable', '!', 'meg', 'ryan', \"n't\", 'even', 'look', 'usual', 'pert', 'lovable', 'self', ',', 'normally', 'make', 'forgive', 'shallow', 'ticky', 'acting', 'schtick', '.']\n",
      "4274975\n"
     ]
    }
   ],
   "source": [
    "# From the word list which is in lower case & stop words are removed\n",
    "# Lemmatize them and store in new List.\n",
    "lmtzr = stem.WordNetLemmatizer()\n",
    "wordList4 = [lmtzr.lemmatize(token) for token in wordList2]\n",
    "\n",
    "print(wordList4[:20])\n",
    "print(len(wordList4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save The Variables using pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. sentences - A list of lists. Each list is further a list of all the sentences.\n",
    "1. wordList - List of all the words\n",
    "2. wordList2 - Stop words removed from it.\n",
    "1. wordList4 - Lemmatized list of all the words.\n",
    "2. lower_tokens - A list of all the lower case words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "outFolder = '..\\\\0.data\\\\processed'\n",
    "\n",
    "senFile = open(outFolder + '\\\\sentences', 'ab')\n",
    "wordFile = open(outFolder + '\\\\wordList', 'ab')\n",
    "\n",
    "pickle.dump(sentences, senFile)\n",
    "pickle.dump(wordList4, wordFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
