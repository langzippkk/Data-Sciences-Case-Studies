{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17500\n",
      "      id  labels                                               text\n",
      "0   1288       0  We saw this on the shelf at the local video st...\n",
      "1   2064       0  Well, you'd better if you plan on sitting thro...\n",
      "2  18997       1  This is my favorite Jackie Chan movie and in a...\n",
      "3  10448       0  The long list of \"big\" names in this flick (in...\n",
      "4  16133       1  The great and underrated Marion Davies shows h...\n",
      "7500\n",
      "      id  labels                                               text\n",
      "0  20594       1  I am decidedly not in the target audience for ...\n",
      "1    602       0  Detective Russell Logan(Lou Diamond Phillips)h...\n",
      "2     29       0  I had some expectation for the movie, since it...\n",
      "3  20342       1  I think that this movie is very neat. You eith...\n",
      "4   6230       0  Well I just gave away 95 minutes and 47 second...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('..\\\\0.data\\\\raw\\\\imdb_train.csv')\n",
    "print(len(train))\n",
    "print(train.head())\n",
    "\n",
    "test = pd.read_csv('..\\\\0.data\\\\raw\\\\imdb_test.csv')\n",
    "print(len(test))\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import stem\n",
    "\n",
    "def clean_paragraph(para):\n",
    "    lmtzr = stem.WordNetLemmatizer()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    p = ' '.join([lmtzr.lemmatize(token.lower()) for token in tokenizer.tokenize(para)])\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.145670652389526\n"
     ]
    }
   ],
   "source": [
    "import time; t0 = time.time()\n",
    "\n",
    "train.text = [clean_paragraph(para) for para in train.text]\n",
    "\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we saw this on the shelf at the local video store saw coppola in the credit and got excited that wa the one and only time this movie raised any interest i could never quite work out if it wa an attempt at a humourous film that failed miserably or an attempt at a serious film that failed miserably in general the entire production seemed incredibly amatuerish the sound in particular wa absolutely dreadful especially in the scene shot in the little bar the dialogue wa so corny in part it wa unbelievable very disappointing'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Classes and Assign Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    8764\n",
      "1    8736\n",
      "Name: labels, dtype: int64\n",
      "0    0.5008\n",
      "1    0.4992\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Find probabilites of each class'''\n",
    "\n",
    "print(train.labels.value_counts())\n",
    "\n",
    "prob_of_each_class = []\n",
    "totalDocCount = len(train)\n",
    "\n",
    "for a in train.labels.value_counts():\n",
    "    prob_of_each_class.append(float(a/totalDocCount))\n",
    "    \n",
    "prob_of_each_class = pd.Series(prob_of_each_class)\n",
    "print(prob_of_each_class); prob_of_each_class.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Word Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58535"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Get Unique Words from the text data'''\n",
    "\n",
    "tokenDict = {}\n",
    "i = 0 \n",
    "for row in train.text:\n",
    "    for token in row.split(' '):\n",
    "        if tokenDict.get(token) == None:\n",
    "            tokenDict[token] = i\n",
    "            i = i + 1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "len(tokenDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x58535 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 0 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "import numpy as np\n",
    "\n",
    "matrix = coo_matrix((3, 58535))\n",
    "matrix = matrix.tocsr()\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rritesh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py:746: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  SparseEfficiencyWarning)\n"
     ]
    }
   ],
   "source": [
    "'''Fill the Matrix'''\n",
    "\n",
    "for index, row in train.iterrows():\n",
    "    for token in row['text'].split(' '):\n",
    "        matrix[row['labels'], tokenDict[token]] = matrix[row['labels'], tokenDict[token]] + 1\n",
    "        matrix[2, tokenDict[token]] = matrix[2, tokenDict[token]] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Verify Matrix'''\n",
    "\n",
    "print(matrix[:, 0].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matrix[:, 0])\n",
    "print(wordCountVector[tokenDict['and']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions on Test Data - Load, Clean, Predict Prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Load Test Data'''\n",
    "test = pd.read_excel('..//0.data//raw/CCC_Test.xlsx')\n",
    "test['Target'] = None\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Clean Test Commentary Text Data'''\n",
    "\n",
    "import time; t0 = time.time()\n",
    "\n",
    "test.Commentary = [clean_paragraph(para) for para in test.Commentary]\n",
    "\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in test.iterrows():\n",
    "    pred_test_prob = 0.0\n",
    "    pred_class = 0\n",
    "    \n",
    "    for target_class in range(0, 4):\n",
    "        product = prob_of_each_class[target_class]\n",
    "        \n",
    "        for token in row['Commentary'].split(' '):\n",
    "            if tokenDict.get(token) == None:\n",
    "                continue\n",
    "                \n",
    "            p_value = matrix[target_class, tokenDict[token]] / wordCountVector[tokenDict[token]]\n",
    "            \n",
    "            if p_value != 0.0:\n",
    "                product = product * p_value\n",
    "                \n",
    "        if product > pred_test_prob:\n",
    "            pred_class = target_class\n",
    "            pred_test_prob = product\n",
    "            \n",
    "    test.at[index, 'Target'] = pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Labels back to Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in test.iterrows():\n",
    "    if row.Target == 0:\n",
    "        test.at[index, 'Target'] = \"Run_Bw_Wickets\"\n",
    "    if row.Target == 1:\n",
    "        test.at[index, 'Target'] = \"Dot\"\n",
    "    if row.Target == 2:\n",
    "        test.at[index, 'Target'] = \"Boundary\"\n",
    "    if row.Target == 3:\n",
    "        test.at[index, 'Target'] = \"Wicket\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('..//5.outputs//output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy so far - 61.828 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Include Over_Run_Total Information Also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby(['Over_Run_Total'])['Target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Over_Run_Total.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create Matrix to store prob info for runs'''\n",
    "\n",
    "matrix_runs_prob = coo_matrix((5, 37))\n",
    "matrix_runs_prob = matrix_runs_prob.tocsr()\n",
    "matrix_runs_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Fill Matrix of Runs'''\n",
    "\n",
    "for index, row in train.iterrows():\n",
    "    matrix_runs_prob[row['Target'], row['Over_Run_Total']] = matrix_runs_prob[row['Target'], row['Over_Run_Total']] + 1\n",
    "    matrix_runs_prob[4, row['Over_Run_Total']] = matrix_runs_prob[4, row['Over_Run_Total']] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matrix_runs_prob[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Make New Predictions'''\n",
    "\n",
    "for index, row in test.iterrows():\n",
    "    pred_test_prob = 0.0\n",
    "    pred_class = 0\n",
    "    \n",
    "    for target_class in range(0, 4):\n",
    "        product = prob_of_each_class[target_class]\n",
    "        run_prob = matrix_runs_prob[target_class, row['Over_Run_Total']] / matrix_runs_prob[4, row['Over_Run_Total']]\n",
    "        \n",
    "        if run_prob != 0.0:\n",
    "            product = product * run_prob\n",
    "        \n",
    "        for token in row['Commentary'].split(' '):\n",
    "            if tokenDict.get(token) == None:\n",
    "                continue\n",
    "                \n",
    "            p_value = matrix[target_class, tokenDict[token]] / wordCountVector[tokenDict[token]]\n",
    "            \n",
    "            if p_value != 0.0:\n",
    "                product = product * p_value\n",
    "                \n",
    "        if product > pred_test_prob:\n",
    "            pred_class = target_class\n",
    "            pred_test_prob = product\n",
    "            \n",
    "    test.at[index, 'Target'] = pred_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in test.iterrows():\n",
    "    if row.Target == 0:\n",
    "        test.at[index, 'Target'] = \"Run_Bw_Wickets\"\n",
    "    if row.Target == 1:\n",
    "        test.at[index, 'Target'] = \"Dot\"\n",
    "    if row.Target == 2:\n",
    "        test.at[index, 'Target'] = \"Boundary\"\n",
    "    if row.Target == 3:\n",
    "        test.at[index, 'Target'] = \"Wicket\"\n",
    "\n",
    "test.to_csv('..//5.outputs//output2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy = 62.45% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
